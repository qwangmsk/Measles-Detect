{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "The measles images used for model training are from two sources: \n",
    "1. IEEE DataPort (DOI: 10.21227/9r41-4x79) at https://ieee-dataport.org/documents/image-dataset-various-skin-conditions-and-rashes\n",
    "2. Mpox Skin Lesion Dataset Version 2.0 (MSLD v2.0) on Kaggle: https://www.kaggle.com/datasets/joydippaul/mpox-skin-lesion-dataset-version-20-msld-v20/data\n",
    "\n",
    "After combining the two datasets, our final collection comprised 2,070 skin images, including 212 depicting measles rashes. The dataset was imbalanced, with ~73.6% White, ~11.2% Black or African American, ~10.6% Hispanic or Latina, ~1% Asian, and the remainder representing other minority groups, including Native Hawaiian. The current implementation did not address this imbalance, which will be remedied in a future version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Config parameters\n",
    "img_size = 224\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "lr = 3e-4\n",
    "model_name = 'deit_base_patch16_224'\n",
    "data_root = 'data'\n",
    "output_dir = 'models_cv'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2070, Measles: 212, Non-measles: 1858\n"
     ]
    }
   ],
   "source": [
    "# ========== LOAD IMAGE PATHS AND LABELS ==========\n",
    "class_map = {'non_measles': 0, 'measles': 1}\n",
    "image_label_pairs = []\n",
    "\n",
    "for cls in class_map:\n",
    "    paths = glob.glob(os.path.join(data_root, cls, '*'))\n",
    "    image_label_pairs.extend([(p, class_map[cls]) for p in paths])\n",
    "\n",
    "# Sort together to avoid misalignment\n",
    "image_label_pairs.sort(key=lambda x: x[0])\n",
    "image_paths, labels = zip(*image_label_pairs)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Total images: {len(image_paths)}, Measles: {labels.sum().item()}, Non-measles: {(labels == 0).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With some images coming from the same individuals, we cannot randomly split at the image level. \n",
    "# Otherwise, the same person’s images could be in both training and validation sets, causing \n",
    "# data leakage and overoptimistic results.\n",
    "import re\n",
    "pattern = re.compile(r\"^(?:MSL|MKP|CHP|CWP|HFMD|HEALTHY)_\\d+_\\d+$\")\n",
    "grp_pat = re.compile(r\"^(?:MSL|MKP|CHP|CWP|HFMD|HEALTHY)_\\d+\")\n",
    "groups = []\n",
    "for path in image_paths:             # Assign identical group ids to the images of the same individuals\n",
    "    filename_no_ext = os.path.splitext(os.path.basename(path))[0]  # Remove extension\n",
    "    if pattern.match(filename_no_ext):\n",
    "        grp_id = grp_pat.findall(filename_no_ext)[0]\n",
    "    else:\n",
    "        grp_id = filename_no_ext\n",
    "    groups.append(grp_id)\n",
    "# print(image_paths)\n",
    "# print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CROSS VALIDATION SPLIT ==========\n",
    "skf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "#for train_idx, val_idx in skf.split(image_paths, labels, groups):\n",
    "for train_idx, temp_idx in skf.split(image_paths, labels, groups):\n",
    "    print(f\"\\n===== Fold {fold} =====\")\n",
    "\n",
    "    # Further split temp_idx into val_idx and test_idx\n",
    "    temp_image_paths = [image_paths[i] for i in temp_idx]\n",
    "    temp_labels = labels[temp_idx]\n",
    "    temp_groups = [groups[i] for i in temp_idx]\n",
    "\n",
    "    skf_temp = StratifiedGroupKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    val_idx_sub, test_idx_sub = next(skf_temp.split(temp_image_paths, temp_labels, temp_groups))\n",
    "    val_idx = [temp_idx[i] for i in val_idx_sub]\n",
    "    test_idx = [temp_idx[i] for i in test_idx_sub]\n",
    "\n",
    "    train_ds = ImageDataset([image_paths[i] for i in train_idx],\n",
    "                            [labels[i].item() for i in train_idx],\n",
    "                            transform=train_transforms)\n",
    "    \n",
    "    test_ds = ImageDataset([image_paths[i] for i in test_idx],\n",
    "                           [labels[i].item() for i in test_idx],\n",
    "                           transform=val_transforms)\n",
    "\n",
    "    val_ds = ImageDataset([image_paths[i] for i in val_idx],\n",
    "                          [labels[i].item() for i in val_idx],\n",
    "                          transform=val_transforms)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)#, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)#, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)#, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Load model\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=2)\n",
    "    \n",
    "    # Fine-tune model head\n",
    "    # model.head = nn.Linear(model.head.in_features, 2)\n",
    "    \n",
    "    # Fine-tune all layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    best_f1 = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "\n",
    "        # ===== Train =====\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        for imgs, lbls in tqdm(train_loader, desc=\"Training\"):\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, lbls)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            train_preds += outputs.argmax(1).cpu().tolist()\n",
    "            train_labels += lbls.cpu().tolist()\n",
    "\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "\n",
    "        # ===== Eval =====\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                outputs = model(imgs)\n",
    "                val_preds += outputs.argmax(1).cpu().tolist()\n",
    "                val_labels += lbls.cpu().tolist()\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_prec, val_rec, val_f1, _ = precision_recall_fscore_support(\n",
    "            val_labels, val_preds, average='weighted')\n",
    "\n",
    "        print(f\"\\nTrain Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Acc: {val_acc:.4f}, Precision: {val_prec:.4f}, Recall: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, f\"fold{fold}_best.pth\"))\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"✅ Saved best model for fold {fold} (F1: {best_f1:.4f})\")\n",
    "\n",
    "    # Load best model for test evaluation\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "    test_preds, test_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in tqdm(test_loader, desc=\"Testing\"):\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            outputs = model(imgs)\n",
    "            test_preds += outputs.argmax(1).cpu().tolist()\n",
    "            test_labels += lbls.cpu().tolist()\n",
    "\n",
    "    test_acc = accuracy_score(test_labels, test_preds)\n",
    "    test_prec, test_rec, test_f1, _ = precision_recall_fscore_support(\n",
    "        test_labels, test_preds, average='weighted')\n",
    "\n",
    "    print(f\"\\nTest Acc: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_rec:.4f}, F1: {test_f1:.4f}\")\n",
    "    \n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer (PyTorch)",
   "language": "python",
   "name": "transformer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
